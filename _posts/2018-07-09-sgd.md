---
layout: post
title: SGD Optimizer - Implementation and Testing
---

In this blog post, I will be explaning the implementation of the SGD Optimizer with and without momentum approach. I will also be explaning the methodology I used for testing the correctness of the optimizer.

<br>
## SGD Optimizer:
<br>

Stochastic Gradient Descent is the one of the basic optimization algorithms that is used for training the Deep Neural Networks. I have implemented the SGD Optimization algorithm with and without momentum method. The finalized class diagram for the VOptimizer class and the TSGD class, which is derived from the VOptimizer class is shown below,

<br>
![SGD Optimizer]({{ site.baseurl }}/images/sgd_class_diagram.jpg)
<br>

Here, ```Step()``` function is implemented in the base class VOptimizer. And the ```UpdateWeights() and UpdateBiases()``` functions are pure virtual functions. So other optimizer classes extending from the base class VOptimizer, for example: TSGD class must implement the ```UpdateWeights() and UpdateBiases()``` functions. The ```fPastWeightGradients and fPastBiasGradients``` store the accumulation of the past weight and past bias gradients respectively.

<br>
## Momentum Update:
<br>

With Stochastic Gradient Descent we don’t compute the exact derivative of our loss function. Instead, we’re estimating it on a small batch. This means we’re not always going in the optimal direction, because our derivatives are 'noisy'. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivative than our noisy calculations. This is one reason why momentum might work better than classic SGD.

The other reason lies in ravines. Ravine is an area, where the surface curves much more steeply in one dimension than in another. Ravines are common near local minimas in deep learning and SGD has troubles getting out of them. SGD will tend to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum. Momentum helps accelerate gradients in the right direction.

Thus, the momentum update is implemented as follows, ( similar to the tensorflow implementation )

```
accumulation = momentum * accumulation + gradient
variable -= learning_rate * accumulation
```

So, one step of update may be performed as,

$$
\begin{align}
\begin{split}
v_t &= \gamma v_{t-1} + \nabla_\theta J( \theta) \\  
\theta &= \theta - \eta v_t
\end{split}
\end{align}
$$

<br>
## Testing the Optimizer:
<br>

And now, for testing the optimizer, there is no exact methodology that can be used. One possible approach would be to test the convergence of the training and testing error during the training procedure. So the unit tests is created as follows,

Let, X = Random Matrix ( nSamples x nFeatures ),<br>
K = Random Matrix ( nFeatures x nOutput ),<br>
Y = X * K ( nSamples x nOutput ) ( Generated one ).<br>

I created a simple 3 layer DeepNet with the following architecture,

<br>
![Testing DNN Architecture]({{ site.baseurl }}/images/sgd_dnn_arch.jpg)
<br>

I created the trainingData and testingData in a similar manner as described above. And trained my DeepNet to learn this linear function mapping i.e. Y = X * K.

Now for testing, one method is to observe the convergence of the training and testing error. They converged very well in a quite number of iterations as below,

<div>
    <a href="https://plot.ly/~ravikiran0606/6/?share_key=lJGwP6gA4wYq23HB78U3YM" target="_blank" title="SGD without Momentum" style="display: block; text-align: center;"><img src="https://plot.ly/~ravikiran0606/6.png?share_key=lJGwP6gA4wYq23HB78U3YM" alt="SGD without Momentum" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="ravikiran0606:6" sharekey-plotly="lJGwP6gA4wYq23HB78U3YM" src="https://plot.ly/embed.js" async></script>
</div>

<div>
    <a href="https://plot.ly/~ravikiran0606/3/?share_key=pQzr1aQJE3K9hLBjWlpJ5W" target="_blank" title="SGD with Momentum" style="display: block; text-align: center;"><img src="https://plot.ly/~ravikiran0606/3.png?share_key=pQzr1aQJE3K9hLBjWlpJ5W" alt="SGD with Momentum" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="ravikiran0606:3" sharekey-plotly="pQzr1aQJE3K9hLBjWlpJ5W" src="https://plot.ly/embed.js" async></script>
</div>

Another method is to create a identity matrix I = Identity Matrix ( batchSize x nFeatures ) and give this as Input to the DeepNet and forward it and get the output at the last layer. Let this output be Y' ( batchSize x nOutput ).<br>

Now, Since the DeepNet is trying to mimic the function Y = X * K, this output Y' should be equal to K. For this to be true, there is one constrain to be satisfied that the batchSize and nFeatures should be equal so that we can construct the Identity Matrix I as a square matrix with diagonal elements being equal to 1.0. And I got the following results by comparing the Y' with K.

<br>
![Testing SGD Optimizer - Relative Error]({{ site.baseurl }}/images/sgd_tests_relative_error.jpg)
<br>


<br>
## References:

1)[ SGD with Momentum. ](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)

2)[ Tensorflow Implementation of SGD Optimizer. ](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)